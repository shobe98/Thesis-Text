\documentclass[12pt]{article}
\usepackage[utf8]{inputenc}
\usepackage[margin=1in]{geometry}
\usepackage{helvet}
\usepackage{tikz}
\usepackage{multirow}
\usepackage{caption}
\usepackage{array}
\usepackage{subcaption}
\usepackage[section]{placeins}
\usepackage[acronym]{glossaries}
\usepackage{setspace}
\usepackage{subfiles}
\usepackage{tikz}
\usepackage{booktabs}

\usepackage{algpseudocode}


\usepackage{amsmath,amssymb,amsthm}

\usepackage[pagebackref]{hyperref}
\hypersetup{
    colorlinks=false,
    linkcolor=blue,
    filecolor=magenta,      
    urlcolor=cyan,
}


\title{%
    Machine Learning Approach to Predict mRNA Isoform Locations from \acrshort{rnaseq} Data }

\author{Andrei Stanciu}




\makeglossaries

\doublespacing
\begin{document}

\maketitle


\begin{center}
    
\thanks{Advised by: \\ Prof. Colin Echeverría Aitken \\ Prof. Marc L. Smith}
    
       \vspace{0.8cm}
     
    
       \includegraphics[width=0.3\textwidth]{vassar-logo.jpg}
            
       Department of Computer Science
    
            
\end{center}

\newpage

\begin{abstract}
While an individual organism's cells hold the same DNA, what makes them different is the expression levels of each gene. A Messenger RNA (mRNA)---a copy of a DNA---is the blueprint that gets translated into multiple copies of a protein. 
A simplifying assumption in many experiments is that the mRNA copies are
homogeneous. However, recent work has demonstrated that many distinct mRNA isoforms are
transcribed from each stretch of DNA. I propose a set of computational tools to aid biologists in identifying novel mRNA isoforms---a way of integrating mRNA heterogeneity in new and existing experiments. I implemented a novel data generation pipeline that mimics datasets obtained from real cell lines. Additionally, I trained and tested machine learning models to identify mRNA isoforms. 
In a traditional biology laboratory, the identification of such isoforms requires significant resources but holds the potential to reveal regulatory mechanisms in gene expression. The
computational methods developed in this study provide a more accessible alternative for isoform
identification. 
\end{abstract}

\newpage
\section{Introduction}


\subsection{History}
DNA  sequencing has seen an astonishing development in availability and processing power in the past century
\cite{Hood2003}. Only in 1953, Watson and Crick \cite{WATSON1953} proposed the now well-known double helix structure of DNA. Soon after, in 1977, Maxam and Gilbert \cite{Maxam1977} described a method of sequencing DNA. Back then, it took Sanger et al. \cite{Sanger1977} roughly one year to sequence the genome of a virus, consisting of 5,375 nucleotides. By 1986, Smith et al. \cite{Smith1986} showcased a tool that would sequence roughly 250 nucleotides an hour. 

At the same time, in the 1940s and the 1950s, the precursors of modern day computers were being created. Since then, the computing power has increased at an exponential rate, as predicted by Moore \cite{Moore2006}. With the computing speed grew the DNA sequencing throughput. While the Human Genome Project took 10 years to complete, Illumina Sequencing \cite{illuminaSeq} was sequencing 18000 human genomes in 2014. This series of breakthroughs serves as the foundation of what is now the 'genomic era', where the average researcher has access to a multitude of genomic data sets and has the power to process them on their personal computers.   

\subsection{Biological Background}
DNA holds a series of properties that makes it suitable for digital analysis. A DNA molecule consists of two chains of nucleotides, also called strands. In particular, the nucleotides that form a single strand of DNA are adenine (A), cytosine (C), guanine (G), and thymine (T), which can come in any combination. These sequences of nucleotides  encode instructions for making the molecules that support life. 


Due to its underlying chemistry, the strand of DNA has two distinct ends called the 5' end and the 3' end. The strand is oriented from the 5' end to the 3' end. Additionally, each nucleotide from one strand pairs with a corresponding nucleotide from the opposite strand. Thus, adenine (A) pairs with thymine (T), and cytosine (C) pairs with guanine (G). This, together with the fact that the two strands of the helix run in opposite (anti parallel) directions, produces for every sequence of DNA a reverse complement sequence. 

Sequences of DNA are copied (transcribed) to RNA and transported outside the nucleus. RNA is a single stranded chain of nucleotides where Thymine (T) is replaced by Uracil (U). \gls{mrna} is the RNA that holds instructions for making protein. Transcription itself is a highly regulated cellular process, responding to various cellular and extracellular signals \cite{Hahn2011}. The number of \acrshort{mrna}s inside the cell varies significantly from gene to gene, and from cell to cell. A transcriptome, or the collection of all mRNAs in a cell with all the non coding RNAs, defines the identity of a cell in multicellular organisms. A schematic of these processes can be found in Figure \ref{image_1}.

\subfile{image_1.tex}

\subsection{Gene Translation}
Translation is the synthesis of protein from \acrshort{mrna}s. This is a highly complex and highly regulated process. Groups of three consecutive nucleotides are called codons. The gene gets translated codon by codon. A start codon, $AUG$, marks the beginning of this process. 61 of the possible codons code for an amino acid, while three of them represent a stop codon. Ribosomes---macromomolecular machines consisting of ribosomal RNA and ribosomal proteins---bind to the mRNA and use the information in it to build the appropriate protein, aminoacid by aminoacid. A lot of the complexity and regulation of translation comes from the fact that the beginning and end of a protein are marked by a start and stop codon respectively on the mRNA. Since the mRNA strands are oriented from their 5' end to their 3' end, the region before the start codon on an mRNA is called 5' \gls{utr} and the region following the stop codon is called 3'\acrshort{utr}. The \gls{cds} is the region  between the two codons and codes for a protein. The subunit of the ribosome is recruited to the 5' end of the mRNA and then scans the 5'UTR until it identifies the AUG (start) codon.  The identification of the AUG codon triggers the assembly of the full ribosome. Only then does the ribosome start building the protein, a process known as elongation. Figure \ref{image_2} shows a schematic of translation initiation. 
 
\subfile{image_2.tex}

The process of scanning for the start codon is challenging and requires help from multiple initiation factors. The length of the 5'\acrshort{utr} can be thousands of nucleotides long. \acrshort{mrna} can bind to itself, forming secondary structures which the ribosome has to unwind for efficient scanning \cite{Kertesz2010}. Dvir et al. \cite{Dvir2013} showed that changing even up to ten nucleotides upstream from the start codon significantly impacts translational efficiency of the gene. Additionally, translation can be influenced by a multitude of factors. It is one of the cell's primary mechanisms to respond to external factors, tuning the expression levels of genes when the environment requires it. 

The 5'\gls{utr} holds another regulatory mechanism that impacts translation globally: upstream Open Reading Frames (uORFs). They can be found in some genes whose 5'\acrshort{utr}s contain specific sequences of nucleotides upstream of the main AUG codon, which begin with a start codon or a near-cognate and end with an in frame stop codon. uORFs can often signal the ribosome to make a protein before it reaches the main coding sequence. A canonical example of how the presence of uORFs affects translation can be seen in gene \textit{GCN4} \cite{Hinnebusch1997}. The gene has four upstream ORFs that work together in inhibiting translation under normal conditions. In other words, under normal conditions, the ribosomes will translate the first uORF and will have enough time to reinitiate by the time they reach the fourth uORF. Only a small number of ribosomes will be able to reinitiate on the main start codon. Under starvation conditions, half of the ribosomes that translated the first uORF will not reinitiate translation on any of the remaining three uORFs, allowing for the actual gene to be expressed. These examples show how the shape of mRNA, together with its uORFs, can have cascading effects in the functionality of a cell. These natural mechanisms demand a new level of rigor that accounts for variations in \acrshort{utr}s when analysing \acrshort{rnaseq} data.

\subsection{RNA Sequencing}
    Understanding and comparing the transcriptomes of different cells under different conditions has become an essential part of recent studies. In order to achieve this, experiments---known as \gls{rnaseq}---are designed such that all RNA in a cell is sequenced with nucleotide precision. \acrshort{rnaseq} has many applications. For instance, Kertesz et al. \cite{Kertesz2010} quantified, with nucleotide precision, the likelihood of mRNA strands to bind to themselves for all yeast genes. Sen et al. \cite{ Sen2015} used \acrshort{rnaseq} and Ribosome Profiling to understand the role of proteins eIF4A and Ded1 in 5'UTR scanning and translation initiation. Kapranov et al. \cite{Kapranov2002} used \acrshort{rnaseq} to classify the transcriptional activity in two human chromosomes. Arzalluz-Luque et al. \cite{ArzalluzLuque2018} described a method of using single cell \acrshort{rnaseq} for identification of RNA isoforms. With the wide availability of high throughput sequencing, it is now possible to sequence all the RNA  present in an experiment \cite{Kukurba2015, Wang2009}. In the current technological era, such an experiment typically gives information about millions of nucleotides present in a cell. 
    
    One common sequencing technology is the Illumina sequencing. It excels at sequencing relatively short mRNA fragments---on the order of hundreds of nucleotides per fragment \cite{Illumina, illuminaSeq}.  The way these experiments are prepared and sequenced involves multiple steps which can each introduce a specific bias. In the case of Illumina sequencing, a biological sample often consists of multiple cells with the same condition. The cells in the sample are broken apart to expose the mRNAs, which are then filtered out. The purified mRNA subsequently gets broken up into smaller fragments. There are different methods available for fragmentation depending on the experimental design and desired outcomes. The fragmented mRNAs are then reverse transcribed to complementary DNA (cDNA), which is augumented with adapters and barcodes. The adapter is used for binding different compounds to the fragments. The barcodes serve as a unique identifier for the experiment. In the case that multiple experiments are sequenced at the same time, the barcodes are used to tell which read comes from which experiment. Subsequently, a PCR reaction amplifies the number of present fragments by making millions of copies of each fragment \cite{Goswami2016}. At this stage of \acrshort{rnaseq}, the multitude of steps are likely to introduce various biases that would favor one category of fragments or genes over another. One such bias in the experiment is the emergence of preferred locations for fragmentation, based on the chemical interactions of specific sequences. Additionally, during the amplification phase of sequencing, different lengths of mRNA can be better competitors for amplification \cite{Lahens2014}. 
    
    Another variation in sequencing methodology comes from paired end versus single end reads. Single end reads are sequences of RNA read from one of the fragments described above. Based on the technology used \cite{Freedman2020}, for each fragment sequenced there is exactly one single end read. In the case of paired end reads, the RNA fragment is sequenced twice, once from each end. This gives two different reads. The sequencer imposes some limits on the length of the reads, thus the selected length of fragments influences the downstream possibilities of analysis. If the selected length is short enough, it is very likely that the two paired reads will overlap. This increases the confidence in each of the reads. If the length of RNA is larger than twice the sequencing length, it becomes unlikely that the two ends will overlap. This last option gives an ordering between pairs of reads and can be used for \textit{de novo} transcriptome assembly \cite{Hlzer2019}.
    
    Another widely used application of \acrshort{rnaseq} is ribosome profiling, proposed by Ingolia et al \cite{Ingolia2012}. By "freezing" the translating ribosomes in place, one can extract and isolate the mRNA fragments which were protected by the ribosome. These ribosome-protected fragments can later be sequenced. Together with an appropriate dataset of mRNA fragments, one can quantify how efficiently a gene is translated (ribosomes / mRNA fragments). This method can be used to understand how different conditions affect translation \cite{Zhong2016, Chothani2019}, to identify novel genes or identify upstream Open Reading Frames (u\acrshort{orf}s).
    
    
\subsection{Isoforms}
Until recently, the lack of scientific evidence led people to assume that the mRNAs coming from a gene have a similar form. However, many genes are known to have various isoforms---slight variation in length or in splicing---especially in more complex organisms \cite{Rees2003, Hunt1992}. Known regions of the genome, called introns, lie completely inside a gene's boundary and are removed from the final mRNA, an event known as splicing. For genes that contain introns, there are multiple ways to conduct splicing. These depend on whether introns are removed or not, and at times, whether the regions between introns are kept or not. These alternative splicing variants of mRNA result in similar proteins with functional variations, known as protein isoforms. 
For each protein isoform there exists an mRNA isoform that generated it. There can also be different mRNA isoforms producing the same protein. This is mainly because the mRNA has two regions, the 5'\acrshort{utr} and the 3'\acrshort{utr}, that do not get translated into protein. Any variation in length between two mRNAs beyond the \gls{cds} results in two different isoforms. Thus, splicing and \acrshort{utr} length variations significantly increases the universe of \acrshort{mrna}s coding for the same gene.

Since the 5'\acrshort{utr} of an mRNA impacts the way a gene is translated through structure and uORFs, understanding the relative distribution of these mRNA isoforms has the potential of providing new insight into gene translation regulation.  While there is significant research in analysing specific protein isoforms, the current methods of analysing isoforms across the whole genome are limited.  It is worth mentioning that there are some alternative methods for identifying isoforms. For instance, UNAGI \cite{Alkadi2020} outperforms the Illumina pipeline when it comes to identifying novel transcripts and does slightly better in terms of identifying isoforms. 

My chosen model organism for implementing and testing isoform prediction is Saccharomyces cerevisiae, as it is extensively studied and is a model organism for eukaryotes, for which I have powerful genetic and biochemical tools. Pelechano and Steinmetz \cite{Pelechano2013} have shown that S. cerevisiae presents an extensive level of heterogeneity in its transcriptome. In other words, there is variability in the lengths of the 5’UTR and the 3’UTR. Unfortunately, Pelechano et al. \cite{Pelechano2013} and other similar studies such as Alkadi et al. \cite{Alkadi2020} require long reads of cDNA, a more complicated library preparation than the one needed for \acrshort{rnaseq}. Thus, \acrshort{rnaseq} holds the potential of making isoform analysis accessible and accurate, without the need for extra lab experimentation. The tool proposed in this paper is a digital solution which can be run on old \acrshort{rnaseq} experiments, conveying new insights. In the case of \acrshort{rnaseq} experiments, it is nontrivial to know from which isoform each short read comes, especially given that these isoforms are present in various distributions under different conditions. 

\subsection{mRNA Isoform  identification}

The precise mapping of \acrshort{rnaseq} reads to the precise isoforms from which they are derived is essential to accurate interpretations of \acrshort{rnaseq}  experiments.
UntilA recent study \cite{Shamir2020} showed that the relative distribution of two isoforms of STAT3 directly influences the expression of a gene correlated with COVID-19 infection. A different study from Kim et al. \cite{Kim2009} shows the importance of understanding isoforms for cancer therapies. One disadvantage of such studies is that they focus on already well-documented isoforms, and the scope is restricted to the genes of interest in the study. This study aims to address this issue by looking at gene isoforms globally, through big data analysis and modern machine learning techniques. 

In order to analyze isoforms, I designed a series of visualisation tools. Additionally, I simulated \acrshort{rnaseq} experiments in order to generate datasets with known isoform distributions. I trained a machine learning model on the new datasets in order to identify isoforms in real experiments. This is a novel tool that integrates isoform analysis where it was previously nonexistent. 

\section{Methods}
\subsection{Datasets}\label{datasets}

The main datasets used to run experiments were a series of eight \acrshort{rnaseq} experiments conducted as part of an ongoing study in the Aitken Lab at Vassar College. These came from four biological conditions: two wildtype strains of S. Cerevisiae and two mutants, each coming in two replicates. Additionally, I used a list of the gene isoforms found by Pelechano et al. \cite{Pelechano2013}. The same study \cite{Pelechano2013} used a separate \cite{Wilkening2013}  \acrshort{rnaseq} experiment as a benchmark for their findings. Given that I used the isoform annotations from Pelechano et al. \cite{Pelechano2013}, I attempted reprocessing their \acrshort{rnaseq} experiment. Unfortunately, due to different versions of tools available, replicating the \acrshort{rnaseq} experiment became a difficult task. For more details see the Discussion (\ref{steinmetz_rna}) section. In order to have a set of labeled isoforms together with \acrshort{rnaseq} data, I generated two \textit{in silico} datasets that replicate patterns from real data. More details on how these datasets were generated can be found in section \ref{generating}.

\subsection{mRNA pre-processing}
Processing an \acrshort{rnaseq} experiment takes a significant number of intermediary steps. In the case of the \textit{in vivo} data I used, the mRNA fragment size was selected to be between 30 and 60 nucleotides long. I used \textit{fastx\_clipper} to remove the standard adapter from the single end reads and \textit{fastx\_trimmer}  to trim the first nucleotide of each read and cap the read length to 45nt. Using \textit{bowtie2}, I aligned all the reads against a library of ribosomal RNA and discarded all the reads that  contained rRNA. Finally, using $tophat$, I aligned the mRNA fragments to the reference genome. The aligned files, in \textit{bam} format, are indexed using \textit{SAM tools}. I used the local cluster computer, which runs a \textit{Ubuntu 18.04.5 LTS} distribution and uses the $SLURM$ workload manager. A flow chart of the data pre-processing can be found in Figure \ref{preprocessing}. All of the scripts used, the annotations, and the index files can be found in the Github repository. 


\subfile{flowchart.tex}

\subsection{Preliminary Analysis}

\subsubsection{Read density}
\label{density_section}
The fundamental data structure used in this study is a read density array. For a genomic region of interest, it stores the number of reads in an experiment that have aligned to every position. Plotting the values of a read density allows me to gain intuition of the patterns of transcription in a particular experiment, to highlight transcribed regions, and to compare them. One important way to use this structure in further analysis is to look at the read density around a particular position. An example of this structure can be found in Figure \ref{density}. Thus, an interval query on this structure is: Given an existing \acrshort{rnaseq} experiment, what is the read density array in the range $[a...b]$ in the genome? 
This prompts me to implement a structure that returns arrays for interval queries in an efficient manner. One bottleneck in processing large \acrshort{rnaseq} files is reading and iterating through all the reads in the file. This process is slow, even when the bam file is indexed. Thus, any naive method that would go through the aligned reads for each query would be too slow. Given that S. Cerevisae's genome is relatively small compared to others (12MB vs 3GB for humans), one can hold into memory the read density across the whole genome.

The chosen data structure used to represent read density is an array of integers, with $density[i]$ meaning the number of RNA reads that have aligned to position $i$ in the genome. Instead of iterating through all the positions inside a fragment and incrementing the density, I designed a faster algorithm. The definitions in Algorithm \ref{algo1} provide an overview of the algorithm. I advise caution when implementing the algorithm in order to obtain a good complexity and iterating through the reads only once.

\subfile{algorithm.tex}

\subfile{read_density_fig.tex}



\subsubsection{Metagene Analysis}\label{metagene_sect}
A different analysis strategy was inspired by my hypothesis and uses heavily the read density data structure described in Section \ref{density_section}. The reads that aligned to a particular region are coming from different copies of the mRNA. Not only are there multiple copies of each mRNA, these copies can come from different isoforms, too. The question that arises from this is: can the presence of different mRNA isoforms be identified from the read density alone? Answering this question leads to one of the main hypotheses in the study. For a gene with two isoforms, one expects to see a significant increase in the density right after the beginning of the shorter isoform. This is because the two isoforms overlap the same \acrshort{cds}, and since one is shorter than the other one, its 5'\acrshort{utr} should be completely included in the longer one's 5'\acrshort{utr}. Thus, any read aligned with the gene after the junction can be from either isoform, while a read aligned with the gene between the beginning of the long isoform and the junction must come from the longer isoform. Figure \ref{twoisoform} shows a randomly generated read distribution that comes from two different isoforms of the same gene. Additionally, the figure illustrates some complexities that come with \acrshort{rnaseq} data. The read density data can be noisy, creating local trends in data that can be confused with isoform junctions. Nonetheless, there is an isoform junction (by construction) at position $1200$. These complexities pose a fundamental question the study aims to answer: Are isoform junctions distinguishable from stochastic noise in \acrshort{rnaseq} data?

\subfile{two-isoform-fig.tex}

To validate this hypothesis, I implemented a metagene analysis method to look at all isoform junctions at once. I define an isoform junction as the position (inside the 5'UTR of a gene) where a shorter isoform begins. For example, in Figure \ref{twoisoform}, position $1200$ is an isoform junction. If a gene has $n$ isoforms that differ in 5'UTR length, $n - 1$ isoform junctions can be obtained. This restricted my dataset to genes that have at least two isoforms. The differences in read density around a junction can often be overlooked because of noise in the data, high frequency of isoform junctions in a relatively small region, or low relative distribution between isoforms (for more details on this see the discussion section (\ref{disc})). I believe that aggregating all the densities around known isoform junctions will amplify any local effect in the individual densities. Thus, I define a window of interest (usually -50nt to +50nt around a junction). A metagene plot can be produced by adding up together the local read densities centered around positions of interest, creating an aggregated image of local trends.  A simplified example of this can be seen in Figure \ref{metagene}. By selecting different sets of positions of interest or changing the size of the window, the algorithm can be easily applied to different scenarios. I used the isoforms identified by Pelechano et al. \cite{Pelechano2013} to define known isoform junctions, and I inspected the datasets described in section \ref{datasets}. In addition to that, I naively picked the same number of random positions in the 5'UTRs of genes to be candidates for a control group. Thus, visualising the metagene plot for all the known isoform junctions in contrast to the metagene for the control group gave supporting arguments to my hypothesis. The main observation was that the metagene of isoform junctions saw a significant increase around the positions of interest. Figure \ref{metagenes} shows metagene plots of real isoform data versus randomly selected positions.  

\subfile{meta-fig.tex}
\subfile{metagenes1.tex}

\subsubsection{Hypothesis test to validate results from Metagene Analysis} \label{ks_test}
In addition to the visual comparison between a metagene of true isoform junctions and random positions, one can define metrics that can be used to statistically verify the differences. In order to introduce such a metric, I defined $S$ to be a set of genomic positions of interest (i.e. isoform junctions). To avoid having overlapping data, I restricted the set of junctions to a randomly selected junction from each gene, rather than considering all junctions annotated in the genome.     
To quantify the significance of this observation, I defined a metric $\delta_n(S)$ for the set $S$ by $\delta_n(S) = metagene(S)[+n] / metagene(S)[-n]$---the ratio between cummulated read densities $n$ basepairs downstream from any point in $S$ and $n$ basepairs upstream. I generated $10000$ control groups $R_i, i \in [1...10000]$. I took advantage of the large computational power at my disposal, and with every iteration, I selected the elements of $S$ again, thus generating a collection of sets $S_i$ where each set contains a junction from every gene. Because of randomness, $S_i$ and $S_j$ will likely include different junction candidates from the same gene. I varied $n$ between $1$ and $100$ and computed $\delta_n(R_i)$ and $\delta_n(S_i)$. With this large amount of simulation data, one can generate the sampling distributions of $\delta(n)$ under different conditions. This allowed me to run a paired Kolmogorov-Smirnov \cite{Massey1951} statistical test and verify my hypothesis. 


\subsection{\textit{in silico} Data Generation}\label{generating}

In order to verify that read density data holds relevant information for identifying isoform distribution, I implemented a tool that generates random reads. The tool uses knowledge of isoform distributions that, in this case, is also randomly generated. The first prototype of data generation starts from the existing annotated CDSes in the genome. For each CDS, I randomly fixed the number of 5'UTR isoforms (sampling uniformly between $1$ and $6$). 

Subsequently, I randomly sampled the length of the 5'UTR for each isoform. In this case, I sampled from the distribution of all 5'UTR lengths identified by Pelechano et al. \cite{Pelechano2013}. To obtain a parametric distribution of this data, I used the python package \textit{fitter} to iterate through multiple candidate distributions and fit them to the data. I selected a gamma distribution \cite{gamma}, which gave the best fit. 

The next step in this process was generating an underlying distribution for the newly generated isoforms. To accomplish this, I generated random weights for each one of the isoforms. Taking the ratio of each individual weight to the total weight of the isoforms gives a relative abundance for each isoform. This method imposes some restrictions on the number of isoforms. More specifically,  as it is often seen in the Pelechano data, a gene will have hundreds of isoforms, out of which only a small number will occur with high frequency. Generating too many isoforms for a gene risks having very low relative frequency among them, making the transition between isoforms indistinguishable from random noise; therefore, for the first model, I decided to generate up to $6$ isoforms for each gene. This often results in different distributions. The spectrum of generated distributions of isoforms ranges from $1$ dominant isoform together with some infrequent isoforms to  all isoforms equally distributed. Additionally, the low number of isoforms minimizes the chance that isoform junctions are clustered together. For potential variations of this model, see the Discussion section (\ref{disc}).

Finally, I randomised the number of reads I saw across all isoforms of a gene. I attempted two different strategies for this subtask. Initially, the number of generated reads was uniformly sampled between $0$ and $5000$ (a number of reads that promise high read densities). The second attempt improved upon the first, introducing a fitted distribution through real read counts in \acrshort{rnaseq} experiments. Given the underlying isoform distribution and the total number of reads in a gene, it was easy to compute how many reads come from each isoform. Thus, the generation problem was reduced to generating a fixed number of reads (intervals) that are fully contained in an annotated region (the isoform---also an interval). Again, I employed a naive approach to this problem. I fixed the read length based on the average read length in my mRNA experiments (44 nt) and I sampled $n$ points from the large interval as the beginning of each read. The end points of each read are $length$ base pairs further downstream from the beginning. In this process, one has to pay attention not to generate reads that fall outside of the isoform boundaries. Section \ref{disc} contains an example of a more sophisticated method for read generation, suggested for future work.  

This subpipeline for data generation has multiple parameters that can be varied. Due to its random nature, each dataset is different; therefore, obtaining large amounts of data for classifier training is feasible. Such a library accounts for varying expression levels across experiments and constructs a fictional database of isoforms with annotated distribution data. To verify my hypothesis holds on this data, I employed the same metagene analysis (Section \ref{metagene_sect}) I used for my \acrshort{rnaseq} data.  


\subsection{Machine Learning Models}

Identifying whether or not an isoform is present at a given position can be framed as a classifying problem. Setting isoform identification as a classifying problem restricts the horizon of Machine Learning models to supervised learning---training and testing data is labeled. Basheer et al. \cite{Basheer2000} reviewed the multitude of classifiers available and focused on an in-depth review of Artificial Neural Networks (ANN) due to their popularity, versatility, and efficiency. For similar reasons, I designed, trained, and tested a backpropagation ANN (BPANN) \cite{Leonard1990} to identify isoform junctions. As Basheer et al. \cite{Basheer2000} noted, a trial and error method is common in order to find the optimal network design. All the BPANNs designed are dense, feed forward networks, have one output node, and have a varied number of hidden layers. The activation functions for the layers were picked between sigmoid and relu. All datasets were randomly split into $60\%$ of the data for training and $40\%$ of the data for testing, in accordance to common practices in the field. Similarly, the error function was fixed to binary corssentropy in all designs, and the training method was fixed to stochastic gradient descent. 


\subsubsection{Dataset \& Feature definition}

The data format used for training the model consists of \textit{in silico} generated read densities paired with a list of isoforms and their relative distributions for every gene present in the yeast genome. As it is, the dataset is not yet suitable for machine learning, as it needs to be adapted to a well-defined problem. Regarding this, I attempted two different problem definitions and ran them on the two generated datasets. 
The first design attempted to predict whether a given position in the genome is an isoform junction based on the read density around that position. Informed by the metagene analysis, I restricted the context of a genomic position to 50 nucleotides upstream and downstream from it. The read density values in this window became the features for the BPANN---an array of $101$ integers. The feature vectors can be indexed from $-50$ to $+50$, corresponding to the distance from the position of interest. The labels are boolean values that affirm whether there is an isoform junction at position $0$ in the feature vector. After labeling, the ratio between isoform junctions and non junction positions was very small. In order to correct for this, only a fraction (5\%-10\%) of the non junction positions were used for training and testing. 

In the second design, I predicted whether an isoform junction was present in a given window. In order to keep the feature vector similar to the one in the previous design, I fixed the window size to $101$ nucleotides. The feature vector was defined as previously, as the read densities in that window. The labels affirmed whether any isoform junctions were present in the window. In this case, the ratio between isoform containing regions and regions that do not contain any isoforms was close to one, so no filtering was required before training.



\section{Results}\label{res}
\subsection{Metagene plots highlights large increases in read densities around isoform junctions}
To understand whether the number of reads increases after an isoform junction, I queried the entire genome for patterns that support this theory. Figure \ref{interesting_genes} shows four genes that present visible increases in read densities. 
Additionally, I took all the isoform junctions annotated by Pelechano et al. \cite{Pelechano2013} and constructed a metagene plot around all the identified junctions. Isoform junctions have a global effect on read densities---the aggregated read counts significantly increase after the junctions. This effect can be seen in Figure \ref{metagenes} (top). 
It is possible that this effect was magnified by having overlapping regions considered in the metagene (i.e., isoform junctions from the same gene that are very close to each other). To remove this source of possible bias, I remade the metagene plot with only one junction from each gene, avoiding overlapping positions entirely. Section \ref{metagene_sect} describes this process in more detail. Figure \ref{good_metagene} shows the metagene plot for one of the sets $S_i$ described before. The same increase in aggregated reads was still prominent after restricting the dataset. As expected, when compared to the metagene  plot of the entire universe of isoforms (including the overlapping regions), the observed effect was less significant.
To verify that the observed increase in aggregated reads was an effect of the junctions and did not apply to the rest of the genome, I generated a metagene plot (Figure \ref{random_metagene}) for randomly selected genomic positions inside 5'\acrshort{utr}s. Sets of random positions did not show the same increase in aggregated read densities as sets of isoform junctions.

\subfile{interesting_genes.tex}
\subfile{good_metagene.tex}
\subfile{random_metagene.tex}

\subsection{Kolmogorov-Smirnov test on metagene data}

While the observable increase in the number of reads in the metagene plots appeared significant, the only way to fully attribute the increase in densities as an effect of isoforms was to run a statistical test. I ran the Kolmogorov-Smirnov test on the distribution of $\delta_n$---the measure for the likelihood of read densities to increase after specific positions (Section \ref{metagene_sect})---for both the generated data and the \acrshort{rnaseq} data. Figures \ref{boxplots_rna} and \ref{boxplots_generated} shows the trend of the distributions as $n$ increases in \acrshort{rnaseq} and generated data respectively. 


For hypothesis testing, I picked a significance threshold of $\alpha = 0.05$. The hypothesis I set up for a particular $n$ can be seen below. Here, I rely on the notation from Section \ref{ks_test}. \\
\\
\textbf{H\textsubscript{0}}: The distribution of $\delta_n(S_i)$ and $\delta_n(R_i)$ are the same. \\ 
\textbf{H\textsubscript{a}}: The distribution of $\delta_n(S_i)$ is shifted to the right of (is greater than) $\delta_n(R_i)$.\\


The results of the statistical test are reported in Table \ref{test_results_rna} for the \acrshort{rnaseq} data. In the case of generated data, the $p$-value is $0$ for any $n$ while the KS-statistic is virtually $1$. The $p$-values for $n \le 50$ are always $0$, with KS-statistic values closer to $1$.  As $n$ increases, the confidence decreases, while still remaining significant. The KS-statistic approaches $0$ as $n$ increases. This means that the difference in distributions of $\delta_n(S_i)$ and $\delta_n(R_i)$ shrinks as $n$ increases.

\subfile{delta_n_figure1.tex}

\subfile{delta_n_figure2.tex}

\subfile{ks_test_bam.tex}

\section{Discussion}\label{disc}
The study provides a set of computational tools that identify \acrshort{mrna} isoforms from \acrshort{rnaseq} data. The machine learning approach serves as a proof of concept and invites future researchers to incorporate isoform data in their studies, given that it was previously overlooked. In implementing the current approach, I provided a starting framework for developing more sophisticated models in the future. 
Throughout the development process, the tools implemented allowed me to ask new questions based on the existing data. By visualising the read densities of the dataset, I identified a gene \textit{EFB1} that overlaps with a small nucleolar RNA location. In the read density, this can be seen as a huge spike in density. In fact, the magnitude of the density is so large that this spike, together with a small number of similar spikes, consists of more than 90\% of the data in chromosome I. I identified other spikes that overlap with known positions of tRNA coding sequences. Some similar spikes can be found outside of coding regions entirely.  These spikes are supported by existing research. While not all the human DNA codes for proteins, more than 90\% of the human genome gets transcribed \cite{Pertea2012}. Classifying and understanding these anomalies will help me further understand the genome. Their identification was only made possible through careful inspection of my data. When I applied Artificial Neural Networks to genomic data, I noticed high ($>75\%$) accuracies in identifying isoforms, even with naive models. This suggests that similar models can be used for other classification tasks as well. In fact, Zhang et al. \cite{Zhang2017} have implemented a Recurrent Neural Network for labeling anomalies in \acrshort{rnaseq} data. Additionally, Hill et al. \cite{Hill2018} have used machine learning for novel transcript identification. Another conclusion that resulted from observing these anomalies is that, in an attempt to filter these outliers, I realised that filtering \acrshort{rnaseq} data is a difficult problem. One reason for this is that the number of reads aligned for each gene follow a Zipf distribution \cite{PhysRevLett.90.088102}. This means that spikes in read density could easily come from genes that are very highly expressed. 
 

Given a set of isoform junctions, the metagene analysis, together with the Kolmogorov-Smirnov test, has proven to be a reliable tool to statistically validate how such a set is different from a random one. One particular weakness of the metagene analysis is that it only conveys information for a set of positions of interest, but not for particular positions. The same analysis can be adapted to individual positions and could be a potential direction for future research. However, the risk associated with this, as mentioned in Section \ref{density_section}, is that the noise in read densities around individual positions could reduce the significance of the test. 

\subsection{Using RNASeq data for model training remains an unaccomplished task} \label{steinmetz_rna}

An initial goal of this study was training the ML model on \acrshort{rnaseq} data that comes from the same biological sample as the TIFSeq data. I attempted to process the \acrshort{rnaseq} data from Pelechano et al. \cite{Pelechano2013} through my own pre-processing pipeline. A reason this became an intractable task is that I had incomplete information on the adapters and barcodes used for sequencing. Additionally, it appears that \textit{tophat} aligned most reads to the genome, but failed to match the alignments of the two reads. In order to guarantee high read scores, one requires detailed information on the experimental design for adapter and barcode clipping. Moreover, using \textit{tophat2} with similar parameters to those used in the past in the lab performed poorly on paired end alignments ($<30\%$ concordant alignment rate). It is still possible that, in future studies, different tools can be used to obtain good alignment rates. Moreover, the remarkable work of Pelechano et al. \cite{Pelechano2013} laid the groundwork for sequencing \acrshort{mrna} isoforms from biological samples. Thus, as a longer term project, one can generate TIFSeq data and \acrshort{rnaseq} data from the same biological replicates. This would generate a dataset with isoform labels on which new models can be trained.   

\subsection{Using Ribosome Profiling to augment RNASeq data}
The technology used for RNA sequencing can be adapted to sequence fragments of mRNA protected by translating ribosomes \cite{Ingolia2012}. Ribosome density data has the potential to improve the efficiency of the model if used as a second feature. Reixachs-Sole et al. \cite{ReixachsSol2020} recently implemented a tool that quantifies isoform distributions with the help of ribosome profiling. Similarly, DeepRibo \cite{Clauwaert2019} is a recent tool that trains a machine learning model on ribosome profiling data. It identifies ORFs, highlighting another possible use of ribosome profiling. Malone et al. \cite{Malone2017} also used unsupervised bayesian learning on ribosome data to predict \acrshort{orf}s. For future development, these predicted \acrshort{orf}s can be incorporated into \acrshort{mrna} isoform identification, either as features to the machine learning model or naively; the presence of an \acrshort{orf} guarantees the existence of an isoform at least as long as the beginning of the \acrshort{orf}.  
 
\subsection{Optimizations and Code Refactoring}

For future work, I propose a series of optimizations to the code. First, a significant number of my tools work for data coming from the positive strand only. This is mainly because there are some inconsistencies across datasets in how segments on the negative strand are represented. For example, the genome annotations treat the negative strand as an independent sequence, resulting in start positions being less than the end positions of given annotations. Conversely, the Pelechano and Steinmetz \cite{Pelechano2013} dataset treats the negative strand as the reverse of the positive strand. The start position therefore has a larger value than the end position. In retrospect, modifying the datasets to use a consistent notation would have been preferred to treating the positive and negative strands as separate cases. Moving forward, changing the codebase to properly analyze negative strand data is not challenging, but requires tracking the data through the pipeline and making small changes in multiple places. Alternatively, one could separate the data  from the beginning into positive and negative strand data and run the pipeline twice on each dataset.

Second, for every experiment I conduct, I generate the read densities. This is a resource-heavy operation since it requires iterating through all the reads in a \textit{bam} file. A first optimization method I suggest is separating the read density generation from the rest of the code and saving them into a file that is then loaded into memory when needed. Moreover, late in my research, I identified the \textit{samtools depth } command that constructs read densities from given \textit{bam} files. Using this program would indeed speed up generation of read densities for \acrshort{rnaseq} data, but in the case of \textit{in silico} generated data having the read density generation implemented in house is more efficient. While \textit{samtools depth} makes read densities reusable---as opposed to parsing the entire bam file every time---the code that processes read densities stays relevant as it plots the read densities and generates metagenes from it. 

Finally, the code base changed substantially over the course of a year. With every new functionality or experiment, I had to adapt the existing code to the new problem while maintaining it backwards compatible. This resulted in a multitude of parameters for each function that turned various functionalities on and off. For future development, I suggest an exhaustive refactoring of the code that would reflect the maturity of the project. As part of this refactoring, I suggest introducing the possibility of using \textit{gff} files rather than \textit{bed} files for genome annotations. This is the default format supported by the Saccharomyces Genome Database which receives continuous updates. 



\subsection{Expanding the data generation model}

\subsubsection{Generating data too similar with real data does not produce satisfying results}
The synthetic data generation can be used as a testing framework for various computational tools, including machine learning models. Before reaching the version presented, I attempted to generate reads across the entire genome using weights naively generated from actual \acrshort{rnaseq} data. In one of theses experiments, I took the probability of a read starting at a given position. I observed that the newly generated data follows very similar patterns to the existing data, mimicking the spikes and the noise in the real data with high fidelity. A subsequent approach I took was removing the regions that have an outstanding number of reads per nucleotide ($99^{th}$ percentile of reads per nucleotide). This approach resulted in data that contained empty regions in the read density inside of annotated genes. These two approaches highlight some dangers of staying too close to real data when generating \textit{in silico} data. Using the distribution of reads across the genome just introduces small levels of random noise to existing data. Using an existing dataset can nonetheless be used for informing parameters for read distributions. Another naive method that remains unexplored consists of averaging out the number of reads before turning them into weights. This method can be attempted with or without outlier filtering in the original dataset. More sophisticated methods could include bayesian learning---a form of unsupervised machine learning---that would learn how to generate data starting from existing parameters \cite{Sauta2020}.

\subsubsection{Improving the sampling distributions positively impacts the quality of generated data}
I identified multiple steps in my data generation pipeline where uniform distributions could cause inconsistencies from real data. Throughout the data generation pipeline, the lengths of the generated fragments were all equal to a fixed value. This value was chosen to be the mean read length in my \acrshort{rnaseq} data. In practice, these lengths can vary according to identifiable distributions. The number of reads in a gene is generated uniformly in a fixed range for all genes. This results in a dataset that contains different gene expression levels than those in real data. The first improvement to the model I implemented was replacing this uniform distribution with the empirical distributions of read lengths, computed from real data. This change is similar to the way I sampled the 5'UTR length of isoforms. Improving the generation model significantly improved the accuracy of the machine learning classifier. This result reinforces the importance of improving the generation model. In the case of the fitted distribution through 5'\acrshort{utr} lengths, the only dataset I had with exhaustive 5'UTR information was the Pelechano et al. \cite{Pelechano2013} dataset. I believe that more general conclusions can be made with access to different datasets. A different method that has often been explored \cite{Lopez2018} in \textit{in silico} data generation is bayesian learning, which could be a more generalizable alternative to my sampling. 

\subsubsection{Isoforms clustered together can be grouped into families of isoforms}
A particularity of the Pelechano \cite{Pelechano2013} dataset is that genes can have hundreds of identified major isoforms. In these cases, the start positions of these isoforms are clustered together. This high variety of isoforms that begin around given positions dilutes the relative distribution of isoforms and makes it more difficult to assign dominant isoforms to a gene. Zinshteyn  et al. \cite{Zinshteyn2017} designed a method that groups isoforms together based on how close their ends are to each other. This method successfully reduced the large universe of \acrshort{mrna} isoforms to a smaller set of isoform families. Without using this tool, I was forced to make a choice regarding the number of isoforms I would simulate for a gene. The trade-off is between fidelity to the real data that gives low and similar density values for each isoform, or a small number of generated isoforms that allow for more variety in isoform distributions. I opted for the latter, since, in practice, grouping isoforms together caused many of them to become dominant, or at least highly expressed ($>10\%$ as opposed to $~1\%$ in the case of large number of isoforms). 

\subsubsection{Simplifying assumptions in data generation can introduce biases}
While the data generation pipeline performed well in practice, I acknowledge a series of simplifying assumptions that I made which have the risk of introducing biases. First, I decided not to focus on the potential impact of the genomic sequence in isoform start positions. This allowed me to focus on the main goal of the study---showing that read densities hold a significant amount of information to inform accurate predictions of isoform start sites. Thus, the isoform classifier could benefit from an expanded set of features that include genomic sequences. Second, I treated the generated variables as independent steps. In practice, the number of reads that aligned to a gene, the gene length, and its 5'UTR length could be mutually dependent. In this case, I believe that more sophisticated instances of unsupervised learning could learn to reproduce these patterns by training on real data. For example, Marouf et al. \cite{Marouf2020} have already implemented a generative adversarial network for generating \textit{in silico} data. Having access to high confidence isoform locations can facilitate new directions for research. Further down the line, when isoform positions are better documented, they can be used to identify whether there are any sequence characteristics that predict them. In turn, known sequence characteristics can improve the confidence with which isoforms are identified. 

\subsubsection{Integrating existing research into the model}
Finally, I designed the data generation model with different existing tools in mind. In particular, for the purpose of identifying differentially expressed genes, there are tools that simulate the number of reads that have aligned to a gene. These tools start from experimental data \cite{Griebel2012, Gerard2020} and use various statistical methods to generate reliable read counts datasets. Plugging in the these read counts into the model should provide a more accurate depiction of how reads are distributed across genes. 
\subsection{Designing an accurate model for isoform prediction}
Running either of the two machine learning models on the first generated dataset resulted in poor performance. This was explained by the low number of reads per gene generated initially. When the dataset was replaced with its improved version---where the number of reads per gene are modeled by an empirical distribution---the accuracy scores improved. Nevertheless, the model that predicted the exact location of isoforms was not performing favorably. On the other hand, when the scope of the prediction was loosened, the model accurately identified the general location of isoforms. Thus, this study provides a framework to train BPANNs on synthetic data and to use their predictive power in \acrshort{rnaseq} analysis. Even so, there is a risk of misclassifying genomic regions as containing isoforms. This misclassification can happen because real data can have local patterns caused by randomness, similar to patterns around isoform junctions. At the same time, the design does not allow for precise classification of regions that do not contain any isoforms. The models can be improved by changing the type of ANN used. Recurrent Neural Networks (RNN) are a variant of ANNs that perform well on sequential data. Long short term memory (LSTM) cells and gated recurrent units (GRU) cells are the most versatile implementations of RNNs.   

\subsection{Future Work}
Throughout the Discussion section, I offered multiple suggestions to continue research on this topic. In order to make things easier for future researchers, I summarise the discussion section as a list of future tasks:

\begin{itemize}
    \item Re-purpose the machine learning model to identify anomalies in RNASeq data. Other ideas include identifying 3'\acrshort{utr} isoforms and finding novel transcripts.
    \item Attempt running the Kolmogorov-Smirnov test on the distributions of $\delta_n$ applied to individual positions. 
    \item Process the \acrshort{rnaseq} data from Pelechano et al. \cite{Pelechano2013} using different pre-processing tools.
    \item Design a biological experiment that runs TIFSeq and \acrshort{rnaseq} on the same replicate. Subsequently use this dataset to train a model on more significant data.
    \item Introduce ribosome profiling as a secondary dataset that augments \acrshort{rnaseq}.
    \item Use identified u\acrshort{orf}s to validate and improve the confidence of identified isoforms.
    \item Adapt the code to run on negative strand data.
    \item Replace the read density generation in the code with the results of \textit{samtools depth}. 
    \item Clean up the codebase and structure it into a python package.
    \item Experiment with generating \textit{in silico} data that closely tracks real \acrshort{rnaseq} data. Outlier filtering can be integrated into these experiments.
    \item Improve the data generation model by replacing uniform distributions with distributions fitted through real data.
    \item Experiment with entirely replacing the generation model by unsupervised learning that picks dependency relations between generated parameters (gene length, 5'UTR length, number of isoforms etc.). 
    \item Apply a model of grouping isoforms together in order to restrict the universe of isoforms identified by Pelechano et al. \cite{Pelechano2013}.
    \item Replace the \acrshort{mrna} fragment generation model by a model that accounts for different read lengths. 
    \item Introduce genomic context of isoform junctions as possible features for identifying new isoforms. 
    \item Integrate previous research that generates \textit{in silico} read counts into the generation model.
    \item Experiment with different methods of labeling the data for Machine Learning model training.
    \item Replace the artificial neural network by a more modern model, such as a recurrent neural network with long short term memory cells, or with gated recurrent unit cells.
\end{itemize}

\subsection{Conclusion}

This study proposes a robust way of digitally analyzing \acrshort{mrna} isoforms. The workflow starts with generating an \textit{in silico} dataset that assumes \acrshort{mrna} heterogeneity. One can then train a machine learning model on this dataset and use it to predict isoform junctions in real data. Finally, by using a metagene analysis, one can verify the significance of the predicted isoform junctions. This computational pipeline can be easily applied to new and existing data alike, and requires no additional experimentation in a biology lab. There are few studies that highlight and analyse \acrshort{mrna} heterogeneity, and these studies require preparing a separate library in order to integrate isoform analysis into the research. These studies have been around since 2013, and still, eight years later, \acrshort{rnaseq} studies do not integrate \acrshort{mrna} heterogeneity in their analysis, which only shows how resource-consuming the available methods are. The computational alternative proposed in this paper can be run on most modern day computers with no extra cost. The nature of the tools developed here allows researchers to modify and adapt them to their particular interests. Additionally, digitally conducting the \acrshort{mrna} isoform analysis takes significantly less time than traditional methods. Further investigation is needed in order to improve the accuracy of the model. This can be accomplished by improving training data and an extending the feature set. The present study emphasizes the importance of introducing isoform analysis in genetic research with the hope of creating new opportunities for augmenting laboratory work with computational solutions. 



\section*{Acknowledgments}

I would like to thank my two advisors, Professor Colin Aitken and  Professor Marc Smith, for all the guidance and mentorship they offered throughout my entire undergraduate career. I am extremely grateful for being part of the Aitken Lab since my first year of college and being introduced into a new field --- biology. This present study was heavily influenced by work conducted by Dr. Lars Steinmetz and Dr. Vicent Pelechano, and I would like to thank them for taking time to answer all my questions and for illuminating aspects of their work.  

My undergrad experience at Vassar wouldn't have been the same without some of the amazing faculty here. With Professor Rui Meireles, Professor Luke Hunsberger, and Professor Simon Ellis, I worked as a Research Assistant and grew as a better computer scientist. 

Finally, I want to thank my family for their love, their kind words, and for supporting me to go to college in the United States. I would like to give some special thanks to my friends with whom I shared countless sweet memories. Thank you to my friends from Vassar, especially Prat, Githu, Arham, and Andrea, and thank you to my friends from home, especially Robert, Cristi, Tudor, and Livia.  Thank you to Lily for her love, patience, support, and for her help in editing this thesis.  


\section*{Links}
\begin{enumerate}
    \item  \url{https://github.com/shobe98/Thesis-Text}
\item \url{https://github.com/shobe98/bio-thesis}

\end{enumerate}


\newacronym{rnaseq}{RNASeq}{RNA Sequencing}
\newacronym{mrna}{mRNA}{Messenger RNA}
\newacronym{cds}{CDS}{Coding Sequence}
\newacronym{utr}{UTR}{Untranslated Region}
\newacronym{orf}{ORF}{Open Reading Frame}


\printglossary[type=\acronymtype,title=Abbreviations]



\bibliography{sp}
\bibliographystyle{ieeetr}



\end{document}



